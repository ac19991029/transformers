import math 
import torch.nn as nn 
from torch.nn import functional as F  
from model.config import * 
from model.transformer_block import TransformBlock  # å¯¼å…¥å‰é¦ˆç¥ç»ç½‘ç»œæ¨¡å—

class GPTModel(nn.Module):
  def __init__(self, max_token_value):
    super().__init__()
    # ğŸ“¢æ³¨æ„ï¼Œè¿™é‡Œçš„ num_embeddings ä¸€èˆ¬ä½¿ç”¨ tokenizer çš„è¯æ±‡è¡¨å¤§å°ï¼Œè¿™é‡Œä¸ºäº†æ–¹ä¾¿è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨äº†ä¸€ä¸ªæ¯”è¾ƒå°çš„å€¼ï¼Œå³è®­ç»ƒé›†æ–‡æœ¬çš„è¯æ±‡è¡¨å¤§å°
    self.token_embedding_lookup_table = nn.Embedding(num_embeddings=max_token_value, embedding_dim=d_model)  # åˆå§‹åŒ–è¯åµŒå…¥æŸ¥æ‰¾è¡¨
    self.transform_blocks = nn.Sequential(*([TransformBlock() for _ in range(num_blocks)] + [nn.LayerNorm(d_model)]))  # åˆå§‹åŒ–transformer blockå±‚
    self.linear_layer = nn.Linear(d_model, max_token_value, é•¿åº¦ä¸è¶…è¿‡ä¸Šä¸‹æ–‡é•¿åº¦
    position_encoding_lookup_table = torch.zebias=False)  # åˆå§‹åŒ–çº¿æ€§å±‚
    
  def forward(self, source, target = None):
    """
    å®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹
    """
    B, T = source.shape  # è·å–è¾“å…¥çš„batch sizeå’Œåºåˆ—é•¿åº¦
    assert T <= context_length  # ç¡®ä¿è¾“å…¥çš„åºåˆ—ros(context_length, d_model)  # åˆå§‹åŒ–ä½ç½®ç¼–ç æŸ¥æ‰¾è¡¨
    position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)  # ç”Ÿæˆä½ç½®åºåˆ—
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # è®¡ç®—ä½ç½®ç¼–ç çš„åˆ†æ¯é¡¹
    position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)  # è®¡ç®—å¶æ•°ä½ç½®çš„ä½ç½®ç¼–ç 
    position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)  # è®¡ç®—å¥‡æ•°ä½ç½®çš„ä½ç½®ç¼–ç 
    position_embedding = position_encoding_lookup_table[:T, :].to(device)  # è·å–ä½ç½®åµŒå…¥
    x = self.token_embedding_lookup_table(source) + position_embedding  # å°†è¯åµŒå…¥å’Œä½ç½®åµŒå…¥ç›¸åŠ 
    x = self.transform_blocks(x)  # é€šè¿‡ Transformer Blockå±‚
    logits = self.linear_layer(x)  # é€šè¿‡çº¿æ€§å±‚

    if target is not None:  # å¦‚æœæœ‰ç›®æ ‡å€¼ï¼Œåˆ™è®¡ç®—äº¤å‰ç†µæŸå¤±
      B, T, C = logits.shape
      logits = logits.view(B * T, C)
      target = target.view(B * T)
      loss = F.cross_entropy(logits, target)
    else:
      loss = None
    return logits, loss
  
  def generate(self, idx, max_new_tokens):
    """
    å®šä¹‰ç”Ÿæˆæ–°tokençš„æ–¹æ³•
    """
    for _ in range(max_new_tokens):  # å¯¹äºæ¯ä¸€ä¸ªæ–°çš„ token
      idx_ctx = idx[:, -context_length:]  # è·å–ä¸Šä¸‹æ–‡
      logits, loss = self(idx_ctx)  # é€šè¿‡æ¨¡å‹è·å– logits
      logits = logits[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ logits
      probs = F.softmax(logits, dim=-1)  # è®¡ç®— softmax æ¦‚ç‡
      idx_next = torch.multinomial(probs, num_samples=1)  # æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·ä¸‹ä¸€ä¸ª token
      idx = torch.cat([idx, idx_next], dim=1)  # å°†æ–°çš„ token æ·»åŠ åˆ°åºåˆ—ä¸­
    return idx
